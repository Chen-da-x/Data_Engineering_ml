{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('ml-latest-small/movies.csv')\n",
    "ratings = pd.read_csv('ml-latest-small/ratings.csv')\n",
    "tags = pd.read_csv('ml-latest-small/tags.csv')\n",
    "tags.head()\n",
    "del tags['timestamp']\n",
    "\n",
    "row_0 = tags.iloc[0]\n",
    "type(row_0)\n",
    "print(row_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.isnull().any().any()\n",
    "tags=tags.dropna()\n",
    "tags.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = movies.merge(tags, on='movieId', how='inner')\n",
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ratings= ratings.groupby('movieId', as_index=False).mean()\n",
    "del avg_ratings['userId']\n",
    "avg_ratings.head()\n",
    "\n",
    "box_office = movies.merge(avg_ratings, on='movieId', how='inner')\n",
    "box_office.tail()\n",
    "\n",
    "is_highly_rated = box_office['rating'] >= 4.0\n",
    "box_office[is_highly_rated][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_genres = movies['genres'].str.split('|', expand=True)\n",
    "movie_genres['isComedy'] = movies['genres'].str.contains('Comedy')\n",
    "movies['year'] = movies['title'].str.extract('.*\\((.*)\\).*', expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test split\n",
    "Along with the rating, there is also a timestamp column that shows the date and time the review was submitted. Using the timestamp column, we will implement our train-test split strategy using the leave-one-out methodology. For each user, the most recent review is used as the test set (i.e. leave one out), while the rest will be used as training data .\n",
    "\n",
    "To illustrate this, the movies reviewed by user 39,849 is shown below. The last movie reviewed by the user is the 2014 hit movie Guardians of The Galaxy. We'll use this movie as the testing data for this user, and use the rest of the reviewed movies as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'] \\\n",
    "                                .rank(method='first', ascending=False)\n",
    "\n",
    "train_ratings = ratings[ratings['rank_latest'] != 1]\n",
    "test_ratings = ratings[ratings['rank_latest'] == 1]\n",
    "\n",
    "# drop columns that we no longer need\n",
    "train_ratings = train_ratings[['userId', 'movieId', 'rating']]\n",
    "test_ratings = test_ratings[['userId', 'movieId', 'rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the dataset into an implicit feedback dataset\n",
    "As discussed earlier, we will train a recommender system using implicit feedback. However, the MovieLens dataset that we're using is based on explicit feedback. To convert this dataset into an implicit feedback dataset, we'll simply binarize the ratings such that they are are '1' (i.e. positive class). The value of '1' represents that the user has interacted with the item.\n",
    "\n",
    "It is important to note that using implicit feedback reframes the problem that our recommender is trying to solve. Instead of trying to predict movie ratings (when using explicit feedback), we are trying to predict whether the user will interact (i.e. click/buy/watch) with each movie, with the aim of presenting to users the movies with the highest interaction likelihood.\n",
    "\n",
    "We do have a problem now though. After binarizing our dataset, we see that every sample in the dataset now belongs to the positive class. However we also require negative samples to train our models, to indicate movies that the user has not interacted with. We assume that such movies are those that the user are not interested in - even though this is a sweeping assumption that may not be true, it usually works out rather well in practice.\n",
    "\n",
    "The code below generates 4 negative samples for each row of data. In other words, the ratio of negative to positive samples is 4:1. This ratio is chosen arbitrarily but I found that it works rather well (feel free to find the best ratio yourself!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "train_ratings.loc[:, 'rating'] = 1\n",
    "\n",
    "# Get a list of all movie IDs\n",
    "all_movieIds = ratings['movieId'].unique()\n",
    "\n",
    "# Placeholders that will hold the training data\n",
    "users, items, labels = [], [], []\n",
    "\n",
    "# This is the set of items that each user has interaction with\n",
    "user_item_set = set(zip(train_ratings['userId'], train_ratings['movieId']))\n",
    "\n",
    "# 4:1 ratio of negative to positive samples\n",
    "num_negatives = 4\n",
    "\n",
    "for (u, i) in tqdm(user_item_set):\n",
    "    users.append(u)\n",
    "    items.append(i)\n",
    "    labels.append(1) # items that the user has interacted with are positive\n",
    "    for _ in range(num_negatives):\n",
    "        # randomly select an item\n",
    "        negative_item = np.random.choice(all_movieIds) \n",
    "        # check that the user has not interacted with this item\n",
    "        while (u, negative_item) in user_item_set:\n",
    "            negative_item = np.random.choice(all_movieIds)\n",
    "        users.append(u)\n",
    "        items.append(negative_item)\n",
    "        labels.append(0) # items not interacted with are negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Collaborative Filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key here is that we don't need the user to interact on every single item in the list of recommendations. Instead, we just need the user to interact with at least one item on the list - as long as the user does that, the recommendations have worked.\n",
    "\n",
    "To simulate this, let's run the following evaluation protocol to generate a list of 10 recommended items for each user.\n",
    "\n",
    "For each user, randomly select 99 items that the user has not interacted with\n",
    "Combine these 99 items with the test item (the actual item that the user interacted with). We now have 100 items.\n",
    "Run the model on these 100 items, and rank them according to their predicted probabilities\n",
    "Select the top 10 items from the list of 100 items. If the test item is present within the top 10 items, then we say that this is a hit.\n",
    "Repeat the process for all users. The Hit Ratio is then the average hits.\n",
    "This evaluation protocol is known as Hit Ratio @ 10, and it is commonly used to evaluate recommender systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_item_set = set(zip(test_ratings['userId'], test_ratings['movieId']))\n",
    "\n",
    "# Dict of all items that are interacted with by each user\n",
    "user_interacted_items = ratings.groupby('userId')['movieId'].apply(list).to_dict()\n",
    "\n",
    "hits = []\n",
    "for (u,i) in tqdm(test_user_item_set):\n",
    "    interacted_items = user_interacted_items[u]\n",
    "    not_interacted_items = set(all_movieIds) - set(interacted_items)\n",
    "    selected_not_interacted = list(np.random.choice(list(not_interacted_items), 99))\n",
    "    test_items = selected_not_interacted + [i]\n",
    "    \n",
    "    predicted_labels = np.squeeze(model(torch.tensor([u]*100), \n",
    "                                        torch.tensor(test_items)).detach().numpy())\n",
    "    \n",
    "    top10_items = [test_items[i] for i in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    if i in top10_items:\n",
    "        hits.append(1)\n",
    "    else:\n",
    "        hits.append(0)\n",
    "        \n",
    "print(\"The Hit Ratio @ 10 is {:.2f}\".format(np.average(hits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ratings = pd.read_csv('ml-latest-small/ratings.csv')\n",
    "#ratings = pd.read_csv('ml-20m/ratings.csv')\n",
    "ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'] \\\n",
    "                                .rank(method='first', ascending=False)\n",
    "\n",
    "train_ratings = ratings[ratings['rank_latest'] != 1]\n",
    "test_ratings = ratings[ratings['rank_latest'] == 1]\n",
    "\n",
    "# drop columns that we no longer need\n",
    "train_ratings = train_ratings[['userId', 'movieId', 'rating']]\n",
    "test_ratings = test_ratings[['userId', 'movieId', 'rating']]\n",
    "\n",
    "train_ratings.loc[:, 'rating'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f866566543ce403d8fb28cb35e368506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a list of all movie IDs\n",
    "all_movieIds = torch.tensor(ratings['movieId'].unique(), device='cuda')\n",
    "\n",
    "# Convert train_ratings to PyTorch tensors\n",
    "user_item_pairs = torch.tensor(    list(zip(train_ratings['userId'], train_ratings['movieId'])), device='cuda')\n",
    "\n",
    "# Placeholders for training data\n",
    "users, items, labels = [], [], []\n",
    "\n",
    "# Create a dictionary for user-item interactions\n",
    "user_item_dict = train_ratings.groupby('userId')['movieId'].apply(set).to_dict()\n",
    "\n",
    "# Convert user-item dictionary to GPU\n",
    "user_item_dict = {k: torch.tensor(list(v), device='cuda') for k, v in user_item_dict.items()}\n",
    "\n",
    "# Negative to positive ratio\n",
    "num_negatives = 4\n",
    "\n",
    "# Start sampling\n",
    "for (u, i) in tqdm(user_item_pairs):\n",
    "    users.append(u.item())\n",
    "    items.append(i.item())\n",
    "    labels.append(1)  # Positive label for interacted items\n",
    "\n",
    "    # Get user's non-interacted items\n",
    "    not_interacted_items = torch.tensor(\n",
    "        list(set(all_movieIds.tolist()) - set(user_item_dict[u.item()].tolist())), device='cuda'\n",
    "    )\n",
    "\n",
    "    # Randomly sample negative items\n",
    "    negative_samples = not_interacted_items[\n",
    "        torch.randint(0, len(not_interacted_items), (num_negatives,))\n",
    "    ]\n",
    "\n",
    "    users.extend([u.item()] * num_negatives)\n",
    "    items.extend(negative_samples.tolist())\n",
    "    labels.extend([0] * num_negatives)\n",
    "\n",
    "# Convert results to PyTorch tensors\n",
    "users = torch.tensor(users, device='cuda')\n",
    "items = torch.tensor(items, device='cuda')\n",
    "labels = torch.tensor(labels, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66c06808515485fad90c35192a7b1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a list of all movie IDs\n",
    "all_movieIds = ratings['movieId'].unique()\n",
    "\n",
    "# Placeholders that will hold the training data\n",
    "users, items, labels = [], [], []\n",
    "\n",
    "# This is the set of items that each user has interaction with\n",
    "user_item_set = set(zip(train_ratings['userId'], train_ratings['movieId']))\n",
    "\n",
    "# 4:1 ratio of negative to positive samples\n",
    "num_negatives = 4\n",
    "\n",
    "for (u, i) in tqdm(user_item_set):\n",
    "    users.append(u)\n",
    "    items.append(i)\n",
    "    labels.append(1) # items that the user has interacted with are positive\n",
    "    for _ in range(num_negatives):\n",
    "        # randomly select an item\n",
    "        negative_item = np.random.choice(all_movieIds) \n",
    "        # check that the user has not interacted with this item\n",
    "        while (u, negative_item) in user_item_set:\n",
    "            negative_item = np.random.choice(all_movieIds)\n",
    "        users.append(u)\n",
    "        items.append(negative_item)\n",
    "        labels.append(0) # items not interacted with are negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9724,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_movieIds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 1025), (45, 1024))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "semantic_embedding = pd.read_csv('output_50.csv')\n",
    "\n",
    "semantic_embedding = pd.DataFrame(semantic_embedding)\n",
    "semantic_embedding['embeddings'] = semantic_embedding['embeddings'].apply(ast.literal_eval)\n",
    "embeddings_df = pd.DataFrame(semantic_embedding['embeddings'].tolist())\n",
    "\n",
    "# Concatenate the embeddings with the original DataFrame, dropping the old embeddings column\n",
    "semantic_embedding = pd.concat([semantic_embedding.drop(columns=['embeddings']), embeddings_df], axis=1)\n",
    "\n",
    "# Drop the 'txt' column from the DataFrame\n",
    "semantic_embedding.drop(columns=['txt'], inplace=True)\n",
    "semantic_embedding_grouped = semantic_embedding.groupby('item_id').mean()\n",
    "semantic_embedding.shape, semantic_embedding_grouped.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semantic_features_dict = semantic_embedding_grouped.set_index('item_id').T.to_dict('list')\n",
    "semantic_features_dict=semantic_embedding_grouped.to_dict('index')\n",
    "semantic_features_dict = {\n",
    "    movie_id: list(features.values()) for movie_id, features in semantic_features_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensTrainDataset(Dataset):\n",
    "    \"\"\"MovieLens PyTorch Dataset for Training\n",
    "    \n",
    "    Args:\n",
    "        ratings (pd.DataFrame): Dataframe containing the movie ratings\n",
    "        all_movieIds (list): List containing all movieIds\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratings, all_movieIds, sematic_features):\n",
    "        self.users, self.items, self.labels = self.get_dataset(ratings, all_movieIds)\n",
    "        self.sematic_features = sematic_features\n",
    "        self.sematic_length = len(next(iter(semantic_features_dict.values())))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        movie_id = self.items[idx].item()\n",
    "        user_id = self.users[idx].item()\n",
    "        if movie_id in self.sematic_features:\n",
    "            semantic_feature = self.sematic_features.get(movie_id)\n",
    "        else:\n",
    "            semantic_feature = np.zeros(self.sematic_length)\n",
    "        return {\n",
    "        'user': torch.tensor(user_id, dtype=torch.long),\n",
    "        'item': torch.tensor(movie_id, dtype=torch.long),\n",
    "        'label': torch.tensor(self.labels[idx], dtype=torch.float),\n",
    "        'semantic_feature': torch.tensor(semantic_feature, dtype=torch.float)\n",
    "    }\n",
    "    \n",
    "    def get_dataset(self, ratings, all_movieIds):\n",
    "        users, items, labels = [], [], []\n",
    "        user_item_set = set(zip(ratings['userId'], ratings['movieId']))\n",
    "\n",
    "        num_negatives = 4\n",
    "        for u, i in user_item_set:\n",
    "            users.append(u)\n",
    "            items.append(i)\n",
    "            labels.append(1)\n",
    "            for _ in range(num_negatives):\n",
    "                negative_item = np.random.choice(all_movieIds)\n",
    "                while (u, negative_item) in user_item_set:\n",
    "                    negative_item = np.random.choice(all_movieIds)\n",
    "                users.append(u)\n",
    "                items.append(negative_item)\n",
    "                labels.append(0)\n",
    "\n",
    "        return torch.tensor(users), torch.tensor(items), torch.tensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(pl.LightningModule):\n",
    "    \"\"\" Neural Collaborative Filtering (NCF)\n",
    "    \n",
    "        Args:\n",
    "            num_users (int): Number of unique users\n",
    "            num_items (int): Number of unique items\n",
    "            ratings (pd.DataFrame): Dataframe containing the movie ratings for training\n",
    "            all_movieIds (list): List containing all movieIds (train + test)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_users, num_items, semantic_embedding, ratings, \n",
    "                 all_movieIds, embedding_dim = 8, semantic_dim = 1024):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(num_embeddings=num_users, embedding_dim=embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=embedding_dim)\n",
    "        \n",
    "        self.semantic_transform = nn.Linear(semantic_dim, embedding_dim)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=embedding_dim * 3, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.output = nn.Linear(in_features=32, out_features=1)\n",
    "\n",
    "        self.ratings = ratings\n",
    "        self.all_movieIds = all_movieIds\n",
    "        self.semantic_embedding=semantic_embedding\n",
    "        \n",
    "    def forward(self, user_input, item_input, semantic_embedding_t):\n",
    "        \n",
    "        # Pass through embedding layers\n",
    "        user_embedded = self.user_embedding(user_input)\n",
    "        item_embedded = self.item_embedding(item_input)\n",
    "\n",
    "        semantic_embedding_t = self.semantic_transform(semantic_embedding_t)\n",
    "\n",
    "\n",
    "        # Concat the two embedding layers\n",
    "        vector = torch.cat([user_embedded, item_embedded, semantic_embedding_t], dim=-1)\n",
    "\n",
    "        # Pass through dense layer\n",
    "        vector = nn.ReLU()(self.fc1(vector))\n",
    "        vector = nn.ReLU()(self.fc2(vector))\n",
    "\n",
    "        # Output layer\n",
    "        pred = nn.Sigmoid()(self.output(vector))\n",
    "\n",
    "        return pred\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        user_input = batch[\"user\"]\n",
    "        item_input = batch[\"item\"]\n",
    "        semantic_input = batch[\"semantic_feature\"]\n",
    "        labels = batch[\"label\"]\n",
    "        \n",
    "        predicted_labels = self(user_input, item_input, semantic_input)\n",
    "        loss = nn.BCELoss()(predicted_labels, labels.view(-1, 1).float())\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(MovieLensTrainDataset(self.ratings, self.all_movieIds\n",
    "                                                ,self.semantic_embedding),\n",
    "                          batch_size=512, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611 193610 (9724,)\n"
     ]
    }
   ],
   "source": [
    "num_users = ratings['userId'].max()+1\n",
    "num_items = ratings['movieId'].max()+1\n",
    "\n",
    "all_movieIds = ratings['movieId'].unique()\n",
    "print(num_users, num_items, all_movieIds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\D.Joker\\anaconda3\\envs\\tf\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:630: Checkpoint directory d:\\Learning\\movieLen\\dataset\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type      | Params\n",
      "-------------------------------------------------\n",
      "0 | user_embedding     | Embedding | 4.9 K \n",
      "1 | item_embedding     | Embedding | 1.5 M \n",
      "2 | semantic_transform | Linear    | 8.2 K \n",
      "3 | fc1                | Linear    | 1.6 K \n",
      "4 | fc2                | Linear    | 2.1 K \n",
      "5 | output             | Linear    | 33    \n",
      "-------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.263     Total estimated model params size (MB)\n",
      "c:\\Users\\D.Joker\\anaconda3\\envs\\tf\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f50d49f7bb4d5b8de927a48c6d86ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\D.Joker\\AppData\\Local\\Temp\\ipykernel_47176\\173147077.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(self.labels[idx], dtype=torch.float),\n",
      "`Trainer.fit` stopped: `max_epochs=7` reached.\n"
     ]
    }
   ],
   "source": [
    "model = NCF(num_users, num_items, ratings=train_ratings, semantic_embedding=semantic_features_dict, all_movieIds=all_movieIds)\n",
    "trainer = pl.Trainer(max_epochs=7, devices=\"auto\", reload_dataloaders_every_n_epochs=True,\n",
    "                     enable_progress_bar=True, logger=False, enable_checkpointing=True)\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391a09b129d24317905cb5d9a6416970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/610 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hit Ratio @ 10 is 0.591803\n"
     ]
    }
   ],
   "source": [
    "# User-item pairs for testing\n",
    "test_user_item_set = set(zip(test_ratings['userId'], test_ratings['movieId']))\n",
    "\n",
    "# Dict of all items that are interacted with by each user\n",
    "user_interacted_items = ratings.groupby('userId')['movieId'].apply(list).to_dict()\n",
    "\n",
    "hits = []\n",
    "for (u, i) in tqdm(test_user_item_set):\n",
    "    interacted_items = user_interacted_items[u]\n",
    "    not_interacted_items = set(all_movieIds) - set(interacted_items)\n",
    "\n",
    "    selected_not_interacted = list(np.random.choice(list(not_interacted_items), 99))\n",
    "    test_items = selected_not_interacted + [i]\n",
    "\n",
    "    semantic_features = [\n",
    "        semantic_features_dict.get(item, np.zeros(1024)) for item in test_items\n",
    "    ]\n",
    "\n",
    "    user_tensor = torch.tensor([u] * 100, dtype=torch.long)\n",
    "    item_tensor = torch.tensor(test_items, dtype=torch.long)\n",
    "    semantic_tensor = torch.tensor(semantic_features, dtype=torch.float)\n",
    "\n",
    "    predicted_labels = model(user_tensor, item_tensor, semantic_tensor).detach().numpy().squeeze()\n",
    "    \n",
    "    top10_items = [test_items[i] for i in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    if i in top10_items:\n",
    "        hits.append(1)\n",
    "    else:\n",
    "        hits.append(0)\n",
    "        \n",
    "print(\"The Hit Ratio @ 10 is {:.6f}\".format(np.average(hits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "max_lines = 30\n",
    "current_line = 0\n",
    "reviews = []\n",
    "\n",
    "with open('movie_dataset_public_final/raw/reviews.json', 'r') as file:\n",
    "    for line in file:\n",
    "        #if current_line >= max_lines:\n",
    "        #    break\n",
    "        data = json.loads(line)\n",
    "        reviews.append(data)\n",
    "        current_line += 1\n",
    "reviews = pd.DataFrame(reviews)\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 'all_movie_ids' 是一个包含所有电影 ID 的列表\n",
    "ratings = pd.read_csv('ml-20m/ratings.csv')  # 读取评分数据\n",
    "all_movie_ids = ratings['movieId'].unique()  # 示例电影 ID 列表\n",
    "\n",
    "# 获取当前 'reviews' 中出现的 'item_id'\n",
    "present_item_ids = reviews['item_id'].unique()\n",
    "\n",
    "# 找出没有出现的 item_id\n",
    "missing_item_ids = set(all_movie_ids) - set(present_item_ids)\n",
    "\n",
    "if missing_item_ids:\n",
    "    print(f\"Missing item_ids: {missing_item_ids}\")\n",
    "else:\n",
    "    print(\"All item_ids are present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie_dataset_public_final/raw/metadata.json', 'r') as file:\n",
    "    metadata = [json.loads(line) for line in file]\n",
    "metadata = pd.DataFrame(metadata)\n",
    "metadata.head()\n",
    "with open('movie_dataset_public_final/raw/survey_answers.json', 'r') as file:\n",
    "    survey_answers = [json.loads(line) for line in file]\n",
    "survey_answers = pd.DataFrame(survey_answers)\n",
    "survey_answers.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_line = 0\n",
    "ratings = []\n",
    "with open('movie_dataset_public_final/raw/ratings.json', 'r') as file:\n",
    "    for line in file:\n",
    "        if current_line >= max_lines:\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        ratings.append(data)\n",
    "        current_line += 1\n",
    "ratings = pd.DataFrame(ratings)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "命中率 (Hit Rate)\n",
    "覆盖率 (Coverage)\n",
    "NDCG (Normalized Discounted Cumulative Gain)\n",
    "AUC (Area Under the Curve)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
